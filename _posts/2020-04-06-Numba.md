---
layout: post
title: "Python高性能时间序列处理 (Pandas, Numpy & Numba JIT)"
subtitle: "High Efficiency Time-series Analysis with Pandas, Numpy and Numba JIT"
author: "Michael Wang"
header-img: "img/post-bg-everbright.jpg"
header-mask: 0.3
mathjax: true
catalog: true
tags:
  - Python
  - Trading
  - Quant
---

*三个关键词：Python，高性能，时间序列*

使用Python的优点：方便、生态丰富，处理时间序列的包多得手到擒来。  
但是，如果用Python处理**一个单张表1000万行、共100+张表的大型时间序列数据集**，缺点明显就是：**慢**。

下面将从基础开始，讲一些“处理心得”，供优化代码时参考。**有基础的朋友可以跳过直接看“Python+高性能”部分。**

-----------------------

## Python+时间序列
Pandas内置了非常丰富、便捷的时间序列操作模块。一般会想到Pandas库中内置的datetime模块，其主要功能：

- 字符串和Timestamp(时间戳)之间方便的转换
  - 可以单个运算，更可以向量化操作整个pd.DataFrame。
- 对Timestamp进行各种Offset(位移)运算
  - 可以方便地加减时间、取月末、周末等任何想要的时间值
- 用resample(重采样)重新设定时间频率
  - 可以增加或减小频率，搭配"ffill/bfill"(向前/后填充)来处理缺失值。
- 按时间轴进行切片、滚动窗口求值等操作
  - 可以用rolling(freq)等接近自然语言的方式完成。

### A Comprehensive Example
*以下是一个例子，强行用到上了上述时间序列的基本操作。*

- 通过`pd.read_csv`读取示例数据。添加参数`parse_dates={'timestamp':[0,2]}`，表示用第一列和第三列数据拼起来得到一个标准的Timestamp，并命名为`'timestamp'`，最后用`index_col='timestamp'`将其设为索引。
  ```py
  df = pd.read_csv('demo.csv',parse_dates={'timestamp':[0,2]},keep_date_col=True,index_col='timestamp')
  ```
  示例数据`df`长成如下样子，只有一列是真正的数据：`df.price`

  | timestamp           |     date |   timeoffset | datetime   |   price |
  |:--------------------|---------:|-------------:|:-----------|-------:|
  | 2020-02-28 09:30:00 | 20200228 |         5400 | 9:30:00    |  14.86 |
  | 2020-02-28 09:30:03 | 20200228 |         5403 | 9:30:03    |  14.85 |
  | 2020-02-28 09:30:06 | 20200228 |         5406 | 9:30:06    |  14.85 |
  | 2020-02-28 09:30:09 | 20200228 |         5409 | 9:30:09    |  14.86 |
  | 2020-02-28 09:30:12 | 20200228 |         5412 | 9:30:12    |  14.87 |

- 2020闰年的2月有29天，而数据是2月28日的数据。可以用`df_raw.index + pd.offsets.MonthEnd() `把日期换到月末试试，得到：
  ```
  DatetimeIndex(['2020-02-29 09:30:00', '2020-02-29 09:30:03',
                ...
                '2020-02-29 14:56:54', '2020-02-29 14:56:57'],
                dtype='datetime64[ns]', name='timestamp', length=4741, freq=None)
  ```

- 观察`timestamp`可知时间间隔约为3s（但不一定是3s），现在可以用`df_raw.resample('1s').last().ffill()`
  把频率提升到1s为间隔，中间缺失的数据向前填充，得到：

  | timestamp           |     date |   timeoffset | datetime   |   price |
  |:--------------------|---------:|-------------:|:-----------|-------:|
  | 2020-02-28 09:30:00 | 20200228 |         5400 | 9:30:00    |  14.86 |
  | 2020-02-28 09:30:01 | 20200228 |         5400 | 9:30:00    |  14.86 |
  | 2020-02-28 09:30:02 | 20200228 |         5400 | 9:30:00    |  14.86 |
  | 2020-02-28 09:30:03 | 20200228 |         5403 | 9:30:03    |  14.85 |
  | 2020-02-28 09:30:04 | 20200228 |         5403 | 9:30:03    |  14.85 |

- 按照时间求每3分钟的price最大值：`df_raw.resample('3min').max()`




------------------------------

## Python+高性能
有了上述操作基础，接下来要对整个表做操作。整个表结构如下：

| timestamp           |     date |   id |   timeoffset | datetime   |   price |
|:--------------------|---------:|-----:|-------------:|:-----------|--------:|
| 2020-02-27 09:30:00 | 20200227 |    1 |         5400 | 09:30:00   |   14.96 |
| 2020-02-27 09:30:03 | 20200227 |    1 |         5403 | 09:30:03   |   14.96 |
|||......||||
| 2020-02-27 09:30:00 | 20200227 |    2 |         5400 | 09:30:00   |   344.98 |
| 2020-02-27 09:30:05 | 20200227 |    2 |         5405 | 09:30:05   |   345.01 |
|||......||||
| 2020-02-27 09:30:00 | 20200227 |    3 |         5400 | 09:30:00   |   3.18 |
| 2020-02-27 09:30:01 | 20200227 |    3 |         5401 | 09:30:01   |   3.18 |
|||......||||

*时间间隔不一定是3s，timeoffset即为秒数，要按照id来分组处理price这列*

假设我们要做的事情是：针对每个id，对价格序列进行滑动窗口计算一个特定值，并返回。  

很自然地想到要先自定义函数`func`，函数中应包含三个步骤：遍历所有时间、建立滑动窗口、计算值。最后，用`df.groupby.apply(func)`来将函数应用到所有id。

这当中，最耗时的步骤当然是`func`。因为对于每张表内的每个id，都要执行一次`func`，整个数据集一共需要执行360000+次，因此对`func`进行的每一毫秒的优化都是值得的。

`func`的不同写法会导致运行速度极大的差异。直接来看调试的结果：

- `df.rolling('1min').apply(func)`: 1 ms  
  乍一看，很快啊？！
  **然鹅，考虑到prase_datetime这一步的速度，就太！慢！了！**   

  别忘了之前读取数据的时候，我们用了`parse_date`来把字符串转换成`Timestamp`。
  实际上，这一步花费的时间需要50ms以上，对内存更是极大的负担。
  因此，果断放弃使用pandas内置的时间类型，换用`timeoffset`这个整数来做时间操作。

- `[func(getIndex(t)) for t in df.timeoffset]`:64 ms  
  其中func和getIndex都是自定义的函数，getIndex负责取出时间窗口，func负责计算。  

  时间上，这种方法还不如使用pandas内置的时间类型（毕竟pandas优化不是瞎吹的），但是运算时节省了内存。
  循环用了列表表达式，比普通的for循环能快那么一点

- `[func(getIndex(t)) for t in df.timeoffset.values]`: 34 ms  
  把`pd.Series`换成`np.array`能直接快一倍。
  
  其原因在于pandas对象的属性和方法太多，生成和遍历一个pandas对象比numpy对象要慢得多，而从pandas到numpy只需要很短的时间（几乎可以忽略不计）。

- `[jit_func(jit_getIndex(t)) for t in df.timeoffset.values]`: 17 ms  
  用`@jit`装饰器编译函数后，数学运算、根据下标查找数组元素、if判断这些操作都变成了C的速度，速度又直接加快一倍

- `jit_func_with_forLoop(df)`: 1 ms  
  放弃使用列表表达式来循环，直接把循环放到函数内部，并用jit编译加速，完美避开了Python循环慢的问题。
  
  此外，还可以在函数内部还优化上下功夫（例如优化算法复杂度，从两重循环$O(N^2)$改为线性时间）。

## Python 循环速度

> 速度图鉴  
> 🐌=Slow, 🐶=Median, 🐶🐶🐶=多核=🦁,   
> 🦁=Fast, ⚡=Extremely Fast 


### 列表表达式 🐶
最初级的办法是使用列表表达式，生成Python内置的Generator，能比For快那么一点地完成循环。**但只能快那么一丁点。**

### 并行处理 🐶🐶🐶
Python原生支持两个并行的库，分别是multiprocessing和threading。多进程可以用到多个CPU内核，多线程可以天然共享内存。这里不多做介绍，可以参考这篇整理：
[Python 多核并行计算](https://abcdabcd987.com/python-multiprocessing/)

更加高级的API是`joblib`，能很方便地写出并行的程序，尤其适合用在`groupby.apply`上。  
使用套路如下：
```py
from joblib import Parallel, delayed
def apply_parallel(df_grouped,func):
    results = Parallel(n_jobs=-1)(delayed(func)(group) for name,group in df_grouped)
    return pd.DataFrame(results)
```
`joblib`会建立子进程、调用多个CPU内核，同时计算自定义的函数`func`，最后将计算结果合并起来以后返回给主程序。注意，这里使用的`func`尽量要避开I/O运算，否则在各个进程之间共享和等待内存资源，反而会减慢速度。

### 向量化 🦁
开头也说了，Python就是写起来快、跑起来慢（尤其是For循环🐌）。  
对此，Python社区的一个解决办法是大量使用vectorize(向量化🦁)。通过`numpy`进行向量化操作甚至能比Cython还要快，可见`numpy`在向量操作上真的是从底层做了优化。  
**然鹅，并不是所有的实际需求都能向量化、并不是所有向量化的操作都能快到令人满意**。如果某一天我们要写一个逻辑复杂的函数`func`，把它转换成一个向量化函数可以花费很大的力气，最终的运算速度很可能不尽如人意。

### Numba Just-in-time 即时编译 ⚡
最后介绍大招，Numba jit即时编译！专治各种不服。  
具体怎么操作，网上有很多教程，[Numba官方文档](http://numba.pydata.org/numba-doc/latest/user/jit.html)说的最清楚。

使用jit的个人经验：
- Numba主要支持的是numpy对象，能不用pandas的就不用。  
  这样，使用`@jit(nopython=True)`或者`@njit`才能发挥最大效果，否则优化非常有限。
- 固定输入输出数据类型  
  用类似C语言的方式定义函数，例如下面这段程序表示输入一个int16和int16一位数组，输出由两个int16数组成的元组：  
  `@njit("UniTuple(int16,2)(int16,int16[:])")`  
  同时，尽量减少内存开销，能用int16的就不要用int32，用uint16表示正整数可以增加一倍的表示范围。
- 有了jit就不要害怕用For和While循环，不要拘泥于Pythonic的编程思路，要换成C。
- 将For循环写在jit编译的函数内部  
  Python循环很慢，用这种方式可以完美避开这个缺点
- 优化循环逻辑  
  多考虑一下：有必要两重循环吗？用移动下标的方法会比两重循环好很多，一般算法的力量是很强大的。
- 

### Modin Pandas 🐶🐶🐶
Modin是一个天然集成并行处理的Pandas库，其基础包是Dask(Windows)/Ray(Linux)，其目标是囊括、并且能自动优化和并行所有的pandas API函数。

很遗憾的是，Modin目前对`df.groupby.apply`这类高级API的优化还很不到位，运行速度和稳定性还不能看。

但是，**Modin对于I/O读写的优化已经非常优秀了**。同样的数据文件（10,000,000+行，3GB，SSD环境），`pandas.read_csv()`读取需要25s，而Modin Pandas读取只需要9s，再转换成pandas DataFrame也只需要3s，有近一倍的速度提升。

Modin的使用方法也很简单，直接`import modin.pandas as pd`就能替换原先的pandas API了。

